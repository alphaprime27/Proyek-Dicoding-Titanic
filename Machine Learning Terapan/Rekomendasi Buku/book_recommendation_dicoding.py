# -*- coding: utf-8 -*-
"""book-recommendation-dicoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PWh77fx3woq27ucOiwXbPw_5iSg4G-vM

# Collaborative & Content Filtered Book Recommendation
Name: Farel Arden

Berdasarkan salah satu artikel dari Universitas Hasanuddin [tautan](https://journal.unhas.ac.id/index.php/jupiter/article/view/1672), membaca buku merupakan hal yang penting untuk dilakukan. Orang - orang yang memilih untuk sering membaca buku memiliki wawasan yang luas. Lewat membaca, kita juga dapat mengetahui, mengenal banyak hal yang sebelumnya belum dikenal dan kita pelajari dan pahami lewat membaca buku.

<img src= "https://edgecappartners.com/wp-content/uploads/2018/09/boook-e1536947008937.jpg" alt ="Movie" style='width: 400px;'>

Melihat pentingnya dampak buku bagi kehidupan kita, kita perlu banyak membaca buku. Ketika kita membaca buku, kita pasti memiliki ketertarikan kepada satu atau beberapa bidang. Dikarenakan banyaknya buku yang telah dan akan terbit, kita membutuhkan sistem rekomendasi yang akan menyaring buku - buku sesuai dengan selera dan ketertarikan kita. Dengan adanya sistem rekomendasi ini, kita tidak perlu lama - lama dalam mencari buku sesuai ketertarikan kita.

Pada latihan kali ini kita akan menggunakan kedua metode yaitu content dan collaborative based filter.
 - Pada **Content Based Flter**, kita akan menggunakan penulis buku menjadi pusat sebagai pusat dari sistem rekomendasi
 - Pada **Collaborative Based Flter**, kita akan menggunakan penilaian dari berbagai pengguna sebagai pusat dari sistem rekomendasi

___
# Importing Libraries and Dataset

## Importing Libraries

Pada notebook ini saya melakukan import pada beberapa libraries, seperti pandas, numpy, tenserflow, dll.
"""

import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""## Importing Dataset

Pada notebook ini, kita hanya akan menggunakan dataframe book dan rating untuk content dan collaborative filtered recommendation kita
"""

book_dataset=pd.read_csv('../input/book-recommendation-dataset/Books.csv')
rating_dataset=pd.read_csv('../input/book-recommendation-dataset/Ratings.csv')

"""Meneliti ukuran dari dataset book """

book_dataset.shape

"""Meneliti ukuran dari dataframe rating"""

rating_dataset.shape

"""___
# Data Preprocessing

Meneliti 5 data teratas dari dataframe rating
"""

rating_dataset.head()

"""Memberi nama header baru pada kolom Book-Rating dan User-ID pada rating_dataset"""

rating_dataset = rating_dataset.rename(columns={'Book-Rating': 'rating','User-ID':'user_id'})

"""Melihat dari dataframe dari rating dan books terbilang banyak, di sini saya hanya mengambil 10000 row dari book dataset dan 5000 row untuk rating dataset"""

book_dataset = book_dataset[:10000]
rating_dataset=rating_dataset[:5000]

"""Cell code di bawah ini dapat dijadikan pertimbangan dalam memproses data, dikarenakan banyak data rating yang bernilai 0, maka ada 2 pilihan, yaitu:
- Membuang semua row yang memiliki nilai 0 pada kolom rating pada rating dataset
- Tetap menggunakan nilai 0 karena 0 bukanlah nilai NaN

Pada notebook kali ini, saya memilih pilihan kedua di mana nilai 0 bukanlah nilai NaN dan tidak membuang row dengan nilai rating 0
"""

# rating_dataset = rating_dataset.drop(rating_dataset[rating_dataset.rating == 0].index)

"""Meneliti 5 data teratas dari dataset rating"""

rating_dataset.head()

"""Meneliti 5 data teratas dari dataset buku"""

book_dataset.head()

"""Memberi nama header baru pada kolom Book-Title, Book-Author, Image-URL-S, Image-URL-S,Image-URL-M,Image-URL-L pada book_dataset"""

book_dataset = book_dataset.rename(columns={'Book-Title': 'book_title','Book-Author':'book_author','Year-Of-Publication':'year_of_publication','Image-URL-S':'Image_URL_S','Image-URL-M':'Image_URL_M','Image-URL-L':'Image_URL_L'})

"""Meneliti id buku - buku apa saja yang di-rate 10 oleh pengguna"""

rating_dataset[rating_dataset.rating == max(rating_dataset.rating)]
best_booksId = rating_dataset.ISBN[rating_dataset.rating == max(rating_dataset.rating)]
best_booksId = list(dict.fromkeys(best_booksId))

"""Meneliti buku - buku apa saja yang di-rate 10 oleh pengguna"""

best_books = []
for i in best_booksId:
    books_name = book_dataset.book_title[book_dataset.ISBN == i]
    best_books.append(books_name)

best_books

"""Meneliti jumlah buku - buku bagus"""

len(best_books)

"""## Univariate Data Analys

Meneliti distribusi rating dari rating dataframe dengan Barplot
"""

count = rating_dataset["rating"].value_counts()
count.plot(kind='bar', title="Rating");
 
plt.show()

"""Meneliti distribusi tahun terbitnya buku dari book dataframe dengan Barplot"""

count = book_dataset["year_of_publication"].value_counts()
count.plot(kind='bar', title="Year of Publication");
 
plt.show()

"""## Multivariate Analysis

Melihat pairplot yang ada pada rating dataset
"""

import seaborn as sns
sns.pairplot(rating_dataset, diag_kind = 'kde')

"""___
# Content Filtered Recommendation System

Content Based Filtering adalah sistem rekomendasi yang merekomendasikan item sesuai dengan item yang disukai oleh pengguna di masa lampau.

Content Based mempelajari profil dan perilaku dari pengguna yang kemudian dari informasi tersebut dianalisa dan diproses sehingga menghasilkan sistem rekomendasi yang baik. Semakin banyak informasi yang diberikan ke sistem ini, maka sistem rekomendasi berbasis content based akan memiliki akurasi yang lebih baik.

## Data Preparation

Di code bawah ini saya men-drop seluruh row yang memiliki nilai NaN
"""

book_dataset = book_dataset.dropna()
rating_dataset = rating_dataset.dropna()

"""Pada cell code di bawah ini, saya melakukan drop atau pembuangan pada row - row yang merupakan duplikasi dari row - row yang lain, sehingga dataset kita tetap memiliki integritas dan tidak berulang"""

rating_dataset = rating_dataset.drop_duplicates()
book_dataset = book_dataset.drop_duplicates()

"""Meneliti deskripsi dari dataset book"""

book_dataset.shape

"""Meneliti deskripsi dari dataset rating"""

rating_dataset.shape

"""Melihat 5 row pertama dari dataset book"""

book_dataset.head()

"""Untuk selanjutnya, kita perlu mengubah dataframe dari buku menjadi sebuah list"""

book_ISBN = book_dataset['ISBN'].tolist()

book_title = book_dataset['book_title'].tolist()

book_author = book_dataset['book_author'].tolist()

book_year_of_publication = book_dataset['year_of_publication'].tolist()

"""Setelah kita membuat list, kita perlu membuat dictionary yang digunakan untuk memnentukan pasangan key-value pada book_ISBN, book_title, book_author, dan book_year_of_publication."""

book = pd.DataFrame({
    'book_ISBN': book_ISBN,
    'book_title': book_title,
    'book_author': book_author,
    'book_year_of_publication': book_year_of_publication
})
book

"""## Modeling

Pada content Based Filtering, kita akan menggunakan TF-IDF Vectorizer untuk membangun sistem rekomendasi berdasarkan penulis buku.

TF-IDF yang merupakan kepanjangan dari Term Frequency-Inverse Document Frequency memiliki fungsi untuk mengukur seberapa pentingnya suatu kata terhadap kata - kata lain dalam dokumen.
Kita umumnya menghitung skor untuk setiap kata untuk menandakan pentingnya dalam dokumen dan corpus. Metode sering digunakan dalam Information Retrieval dan Text Mining.

Pada cell code di bawah ini kita akan mengambil kata - kata penting dalam kolom book_author
"""

from sklearn.feature_extraction.text import TfidfVectorizer
 
tf = TfidfVectorizer()
 
tf.fit(book['book_author']) 
 
tf.get_feature_names()

"""Kemudian kita akan lakukan fit dan transformasi ke dalam matriks, pada code di bawah ini, matriks tersebut adalah tfidf_matrix"""

tfidf_matrix = tf.fit_transform(book['book_author']) 
 
tfidf_matrix.shape

"""Pada tfidf_matrix terdapat 10000 ukuran data dan 5575 nama penulis buku

.todense(), atribut ini dipakai untuk mengubah tfidf_matrix yang awalnya vektor menjadi matriks
"""

tfidf_matrix.todense()

"""Dataframe di bawah ini digunakan untuk melihat matriks dari judul buku dengan penulis - penulis buku"""

pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=book.book_title
).sample(10, axis=1,replace=True).sample(10, axis=0)

"""Dalam sistem rekomendasi, kita perlu mencari cara supaya item yang kita rekomendasikan tidak terlalu jauh dari data pusat, oleh karena itu kita butuh derajat kesamaan pada item, dalam proyek ini, buku dengan derajat kesamaan antar buku dengan cosine similarity"""

from sklearn.metrics.pairwise import cosine_similarity
 
cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

"""Pada code di bawah ini kita akan membuat dataframe cosine_sim_df dengan baris dan kolomnya adalah judul dari buku"""

cosine_sim_df = pd.DataFrame(cosine_sim, index=book['book_title'], columns=book['book_title'])

"""Di bawah ini adalah fungsi untuk mendapatkan rekomendasi berbasis penulis buku dengan k sebagai jumlah rekomendasi yang diingkan, dalam fungsi ini, kita akan mendapatkan 5 rekomendasi

Atribut argpartition berguna untuk mengambil sejumlah nilai k, dalam fungsi ini 5 tertinggi dari tingkat kesamaan yang berasal dari dataframe cosine_sim_df.
"""

def author_recommendations(i, M, items, k=5):
    ix = M.loc[:,i].to_numpy().argpartition(range(-1,-k,-1))
    closest = M.columns[ix[-1:-(k+2):-1]]
    closest = closest.drop(i, errors='ignore')
    return pd.DataFrame(closest).merge(items).head(k)

"""Pada code cell di bawah ini, kita akan mencari rekomendasi dari buku yang sudah dibaca, dalam kasus ini, buku yang sudah dibaca adalah "The Diaries of Adam and Eve" yang ditulis oleh Mark Twain dan terbit pada tahun 1998"""

books_that_have_been_read = "The Diaries of Adam and Eve"
book[book.book_title.eq(books_that_have_been_read)]

"""Pada cell code di bawah ini, kita akan mendapatkan 5 rekomendasi dari buku "The Diaries of Adam and Eve""""

recommendations = author_recommendations(books_that_have_been_read, cosine_sim_df, book[['book_title', 'book_author']])

"""Pada beberapa kasus, rekomendasi akan memberikan rekomendasi yang terduplikat, sehingga perlu dibuang rekomendasi yang terduplikat"""

recommendations = recommendations.drop_duplicates()

"""Berikut adalah 5 buku rekomendasi yang ditulis oleh Mark Twain"""

recommendations

"""## Evaluation

Kita akan memakai metrik evaluasi akurasi di mana akurasi adalah:

Jumlah buku yang direkomendasikan sesuai dengan penulis buku / Jumlah buku yang direkomendasikan

Variabel books_that_have_been_read_row di bawah ini akan mengambil satu row dari buku yang pernah dibaca sebelumnya, dan variabel books_that_have_been_read_author adalah penulis buku dari buku yang pernah dibaca sebelumnya
"""

books_that_have_been_read_row = book_dataset[book_dataset.book_title == books_that_have_been_read]
books_that_have_been_read_author = books_that_have_been_read_row.iloc[0]["book_author"]

"""Variabel book_recommendation_authors merupakan sebuah list yang terdiri dari penulis - penulis dari buku - buku yang direkomendasikan oleh sistem"""

book_recommendation_authors = recommendations.book_author

"""Code di bawah ini merupakan proses manual di mana setiap penulis dari buku yang direkomendasikan akan dicek, apabila sama, maka variabel real_author akan bertambah 1"""

real_author = 0
for i in range(5):
    if book_recommendation_authors[i] == books_that_have_been_read_author:
        real_author+=1

"""Code di bawah ini adalah akurasi dari model sistem rekomendasi, di mana Jumlah buku yang direkomendasikan sesuai dengan penulis buku (Variabel real_author) / Jumlah buku yang direkomendasikan (5)"""

Accuracy = real_author/5*100
print("Accuracy of the model is {}%".format(Accuracy))

"""Maka dapat dilihat pada akurasi di atas, model menunjukkan akurasi 100%

___
# Collaborative Filtered Recommendation System

Collaborative Based Filtering adalah sistem rekomendasi berdasarkan pendapat suatu komunitas.

Kelebihan pada Collaborative Based Filtering bila dibandingkan dengan Content Based Filtering adalah pengguna dapat mengeksplorasi item atau konten di luar preferensi pengguna. Pengguna pun juga dapat mendapat rekomendasi sesuai dengan kecenderungan publik yang dianalisa lewat penilaian pengguna - pengguna lainnya.

Kekurangan pada Collaborative Based Filtering adalah pengguna kurang mendapatkan rekomendasi sesuai preferensi pribadi. Konten - konten yang diberikan oleh sistem rekomendasi lebih banyak berasal dari preferensi publik dan bukan preferensi pribadi.

Pada Collaborative Based Filtering, saya menggunakan penilaian dari pengguna - pengguna untuk mendapatkan rekomendasi buku - buku.

## Data Preparation

Pada cell code di bawah ini, saya akan meyandikan user_id menjadi integer
"""

user_ids = rating_dataset['user_id'].unique().tolist()

user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

"""Pada cell code di bawah ini, saya akan meyandikan book_id menjadi integer"""

book_ids = rating_dataset['ISBN'].unique().tolist()
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}

rating_dataset['user'] = rating_dataset['user_id'].map(user_to_user_encoded)
rating_dataset['book'] = rating_dataset['ISBN'].map(book_to_book_encoded)

"""Terakhir, kita akan cek jumlah pengguna dan jumlah buku, serta mengubah tipe data rating menjadi float"""

num_users = len(user_encoded_to_user)
print(num_users)
num_book = len(book_encoded_to_book)
print(num_book)
rating_dataset['rating'] = rating_dataset['rating'].values.astype(np.float32)

min_rating = min(rating_dataset['rating'])
max_rating = max(rating_dataset['rating'])
 
print('Number of User: {}, Number of Resto: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

"""## Membagi Dataset

Sebelum kita membagi dataset menjadi data latih dan data validasi, kita terlebih dahulu harus mengacak dataset
"""

rating_dataset = rating_dataset.sample(frac=1, random_state=42)
rating_dataset

"""Pada cell code di bawah ini, saya membagi dataset yang ada menjadi 70% untuk latihan dan 30% untuk validasi"""

x = rating_dataset[['user', 'book']].values
 
y = rating_dataset['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
train_indices = int(0.70 * rating_dataset.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""## Model Development

Model yang akan kita pakai dalam sistem rekomendasi berbasis pendapat pengguna adalah RecommenderNet
"""

from tensorflow import keras
from tensorflow.keras import layers
import tensorflow as tf

class RecommenderNet(tf.keras.Model):
 
  def __init__(self, num_users, num_resto, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_resto = num_resto
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)
    self.resto_embedding = layers.Embedding( 
        num_resto,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.resto_bias = layers.Embedding(num_resto, 1) 
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    resto_vector = self.resto_embedding(inputs[:, 1])
    resto_bias = self.resto_bias(inputs[:, 1]) 
 
    dot_user_resto = tf.tensordot(user_vector, resto_vector, 2) 
 
    x = dot_user_resto + user_bias + resto_bias
    
    return tf.nn.sigmoid(x)

"""Selanjutnya kita melakukan proses compile pada model dengan binary crossentropy sebagai loss function, adam sebagai optimizer, dan RMSE sebagai metrik dari model"""

model = RecommenderNet(num_users, num_book, 50)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Selanjutnya kita akan melatih model dengan batch_size 5 dan 20 epochs"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 5,
    epochs = 20,
    validation_data = (x_val, y_val)
)

"""## Visualisasi Metrik

Berikut adalah hasil latihan dari data yang ada, evaluasi metrik yang digunakan adalah RMSE
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## Mendapatkan Rekomendasi

Sebelumnya, kita perlu mendefinisikan ulang book_datase dan rating_dataset
"""

book_dataset =  book
rating_dataset = rating_dataset

"""Pada cell code di bawah ini, kita akan mengambil user_id secara acak dari rating_dataset. Dari user_id ini kita perlu mengetahui buku - buku apa saja yang pernah dibaca dan yang belum pernah dibaca, sehingga kita hanya dapat merekomendasikan buku - buku yang belum dibaca."""

user_id = rating_dataset.user_id.sample(1).iloc[0]
books_have_been_read_by_user = rating_dataset[rating_dataset.user_id == user_id]
 
books_have_not_been_read_by_user = book_dataset[book_dataset['book_ISBN'].isin(books_have_been_read_by_user.ISBN.values)]['book_ISBN'] 
books_have_not_been_read_by_user = list(
    set(books_have_not_been_read_by_user)
    .intersection(set(book_to_book_encoded.keys()))
)
 
books_have_not_been_read_by_user = [[book_to_book_encoded.get(x)] for x in books_have_not_been_read_by_user]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(books_have_not_been_read_by_user), books_have_not_been_read_by_user)
)

"""Pada code cell terakhir di bawah ini, kita akan merekomendasikan 10 buku dari user"""

ratings = model.predict(user_book_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(books_have_not_been_read_by_user[x][0]) for x in top_ratings_indices
]
 
top_books_recommended = (
    books_have_been_read_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)
 
books_row = book_dataset[book_dataset['book_ISBN'].isin(top_books_recommended)]
for row in books_row.itertuples():
    print(row.book_title, ':', row.book_author)
 
print('----' * 8)
print('Top 10 Book Recommendation for user: {}'.format(user_id))
print('----' * 8)
 
recommended_books = book_dataset[book_dataset['book_ISBN'].isin(recommended_book_ids)]
for row in recommended_books.itertuples():
    print(row.book_title, ':', row.book_author)

"""Berikut adalah sistem rekomendasi dari content dan collaborative based system. Keduanya unik dan memiliki kelebihan dan kekurangannya masing - masing."""